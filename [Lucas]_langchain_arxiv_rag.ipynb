{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "private_outputs": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Task\n",
    "I'd like to build a langchain pipeline that given a relevant user query\n",
    "1. Invokes an arxiv_search tool that searches arxiv for papers related to keywords relevant to the user query\n",
    "2. Downloads the top pdf and puts it into an ephemeral in-memory vector database (e.g. FAISS) using an embedding of your choice\n",
    "3. Provides snippets from said pdf as input to the LLM for final response generation.\n",
    "\n",
    "## Examples\n",
    "1. \"what's the mmlu value of reka core according to the paper on arxiv?\"\n",
    "  - Returns something like \"The score is 83.2\", or\n",
    "  - \"The MMLU score is not in the provided context\" (if retrieval/arxiv search did not work -- which is perfectly fine for the sake of the exercise)\n",
    "2. \"tell me a joke\"\n",
    "  - No tool is invoked; just provide the plain llm response.\n",
    "\n",
    "I kept some code around that starts a background task for ollama, and downloads Gemma2 9B as well as a generic sentence encoder for your convenience, but feel free to pick whatever you'd like.\n",
    "\n",
    "Bonus: Install local phoenix tracing and share a trace.\n",
    "\n",
    "## Overall Goal\n",
    "- I don't care if the particular example queries actually work, I really just want to see the ~20 lines of relevant python code to illustrate the approach.\n",
    "- Colab's free T4 GPU instances should work just fine, but let me know if you run into problems there.\n",
    "- Please don't spend more than 30 minutes on it :)"
   ],
   "metadata": {
    "id": "YvBHGphS5bcj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "id": "yXQ1AXUnLfpp"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Uc4Jzw2sAdhD",
    "ExecuteTime": {
     "end_time": "2024-07-25T04:09:31.503117Z",
     "start_time": "2024-07-25T04:04:54.658202Z"
    }
   },
   "source": [
    "# !pip install langchain langchainhub langchain-community langchain-experimental langchain-huggingface --quiet\n",
    "!pip install arxiv pymupdf --quiet\n",
    "!pip install sentence_transformers --quiet\n",
    "!pip install faiss-gpu --quiet\n",
    "!pip install arize-phoenix --quiet\n",
    "!apt install curl\n",
    "\n",
    "import subprocess"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script pymupdf.exe is installed in 'C:\\Users\\Cinti\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script isympy.exe is installed in 'C:\\Users\\Cinti\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\Cinti\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\Cinti\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\Cinti\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\n",
      "ERROR: No matching distribution found for faiss-gpu\n",
      "  WARNING: The script mako-render.exe is installed in 'C:\\Users\\Cinti\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts cygdb.exe, cython.exe and cythonize.exe are installed in 'C:\\Users\\Cinti\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script uvicorn.exe is installed in 'C:\\Users\\Cinti\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script strawberry.exe is installed in 'C:\\Users\\Cinti\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script httpx.exe is installed in 'C:\\Users\\Cinti\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script alembic.exe is installed in 'C:\\Users\\Cinti\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts opentelemetry-bootstrap.exe and opentelemetry-instrument.exe are installed in 'C:\\Users\\Cinti\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "'apt' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "# !curl -fsSL https://ollama.com/install.sh | sh",
   "metadata": {
    "id": "b5zhg4AFLj4U"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# subprocess.Popen([\"ollama\", \"serve\"], start_new_session=True)",
   "metadata": {
    "id": "wtK_maysLqiO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import phoenix as px\n",
    "session = px.launch_app()"
   ],
   "metadata": {
    "id": "2fVDCICzr5cH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# !ollama pull gemma2:9b",
   "metadata": {
    "id": "BWjHiLFAQL_T"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Arxiv RAG chain"
   ],
   "metadata": {
    "id": "-WJeg08Kn2lO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# import subprocess\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "import phoenix as px\n",
    "session = px.launch_app()\n",
    "!ollama pull gemma2:9b"
   ],
   "metadata": {
    "id": "s2BehtVjpyJJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import arxiv\n",
    "import langchain\n",
    "import faiss\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import fitz\n",
    "import requests\n",
    "import numpy as np"
   ],
   "metadata": {
    "id": "1RQQ5bkUqBXt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# llm = OllamaFunctions(model=\"gemma2:9b\", format=\"json\", temperature=0)\n",
    "embeddings_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embedder = HuggingFaceEmbeddings(model_name=embeddings_model_name, model_kwargs={\"device\": \"cuda\"})"
   ],
   "metadata": {
    "id": "wTiIVXe1qBay"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # Define the function to search arXiv and get the top paper\n",
    "def search_arxiv(query):\n",
    "    search = arxiv.Search(query=query, max_results=1, sort_by=arxiv.SortCriterion.Relevance)\n",
    "    paper = next(search.results())\n",
    "    return paper.pdf_url\n",
    "\n",
    "# Define the function to download and read the PDF\n",
    "def download_pdf(pdf_url):\n",
    "    response = requests.get(pdf_url)\n",
    "    with open(\"temp.pdf\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    doc = fitz.open(\"temp.pdf\")\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Define the function to embed and store in FAISS\n",
    "def embed_text_to_faiss(text, embedder):\n",
    "    sentences = text.split(\". \")\n",
    "    embeddings = embedder.embed_documents(sentences)\n",
    "    embeddings = np.array(embeddings).astype('float32')\n",
    "    faiss_index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    faiss_index.add(embeddings)\n",
    "    return faiss_index, sentences\n",
    "\n",
    "# Define the function to query the FAISS index\n",
    "def query_faiss(query, faiss_index, sentences, embedder):\n",
    "    query_embedding = embedder.embed_query(query)\n",
    "    query_embedding = np.array(query_embedding).astype('float32').reshape(1, -1)\n",
    "    D, I = faiss_index.search(query_embedding, k=1)\n",
    "    return sentences[I[0][0]]"
   ],
   "metadata": {
    "id": "e-s580gYqBdz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # Main function to handle the pipeline\n",
    "def main_pipeline(query):\n",
    "    if \"arxiv\" in query:\n",
    "        pdf_url = search_arxiv(query)\n",
    "        pdf_text = download_pdf(pdf_url)\n",
    "        faiss_index, sentences = embed_text_to_faiss(pdf_text, embedder)\n",
    "        response = query_faiss(query, faiss_index, sentences, embedder)\n",
    "    else:\n",
    "        response = llm(query)\n",
    "    return response"
   ],
   "metadata": {
    "id": "-JnyWtMZqBgm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # Example\n",
    "query = \"what's the mmlu value of reka core according to the paper on arxiv?\"\n",
    "response = main_pipeline(query)\n",
    "print(response)"
   ],
   "metadata": {
    "id": "5WPqZXXLqVnL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# # OK I'll stop here.",
   "metadata": {
    "id": "AKIuS7jjqVpp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Vj3r24TytSwp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "rSM5lK_ctIVK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "OL2SXv9RtIZ0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "r4rU5vQUtIcn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "TuitDglUqVsH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "HK8n-SUBqVuf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "LxVhAE4wqVw5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Bd2Zz4sBqBj8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "# from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "# # # Other langchain import..."
   ],
   "metadata": {
    "id": "wjPxmZGxj3Yf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # llm = OllamaFunctions(model=\"gemma2:9b\", format=\"json\", temperature=0)\n",
    "# \n",
    "# embeddings_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "# embeddings_model_kwargs = {\"device\": \"cuda\"}\n",
    "# embedder = HuggingFaceEmbeddings(model_name=embeddings_model_name, model_kwargs=embeddings_model_kwargs)"
   ],
   "metadata": {
    "id": "BvF4scBEn6-Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "tQuBxnFjhb_v"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
